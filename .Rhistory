temp_dat<-strsplit(final_dat$postdate,split='"')
temp_postdate<-data.frame()
temp_blogger<-data.frame()
for (j in 1:2000){
if(j%%2==0){
temp_postdate2<-temp_dat[[j]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
}else{
temp_blogger2<-temp_dat[[j]][1]
temp_blogger<-rbind(temp_postdate,temp_postdate2)
}
}
temp_postdate2<-temp_dat[[2]][1]
temp_postdate2
rm(list=ls())
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
final_dat = data.frame(alltxt, stringsAsFactors = F)
library(rvest)
library(xml2)
url<-"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=142384&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page="
#100페이지까지 총 1000개의 리뷰
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
final_dat = data.frame(alltxt, stringsAsFactors = F)
temp_dat<-strsplit(final_dat$postdate,split='"')
temp_dat
temp_postdate<-data.frame()
temp_blogger<-data.frame()
for (j in 1:2000){
if(j%%2==0){
temp_postdate2<-temp_dat[[j]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
}else{
temp_blogger2<-temp_dat[[j]][1]
temp_blogger<-rbind(temp_postdate,temp_postdate2)
}
}
temp_dat<-strsplit(final_dat$postdate,split='"')
temp_postdate<-data.frame()
temp_blogger<-data.frame()
for (j in 1:2000){
if(j%%2==0){
temp_postdate2<-temp_dat[[j]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
}else{
temp_blogger2<-temp_dat[[j]][1]
temp_blogger<-rbind(temp_postdate,temp_postdate2)
}
}
temp_dat
temp_dat[[2]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
temp_postdate2<_temp_dat[[2]][1]
temp_postdate2<-temp_dat[[2]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
temp_postedate
temp_postdate
temp_postdate2<-temp_dat[[4]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
temp_postdate2
temp_postdate
temp_postdate2
temp_postdate2<-temp_dat[[2]][1]
temp_postdate<-rbind(temp_postdate,temp_postdate2)
dim(temp_postdate)
temp_postdate<-data.frame()
temp_postdate<-rbind(temp_postdate,temp_postdate2)
dim(temp_postdate)
temp_postdate2<-temp_dat[[4]][1]
temp_postdate2
temp_postdate<-rbind(temp_postdate,temp_postdate2)
alltxt
head(alltxt)
head(alltxt$postdate)
postdate
postdate[1]
length(postdate)
length(alltxt)
final_dat = data.frame(alltxt, stringsAsFactors = F)
final_dat$postdate
final_dat$postdate
stringr::str_match_all(final_dat,"2017")
final_dat
class(fin'')
class(final_dat)
head(final_dat)
final_dat[1,]
a<-final_dat
head(final_dat)
postdate
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
alltxt
txt
postdate
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
postdate<-postdate[c(2,4,6,8,10,12,14,16,,18,20)]
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
url<-"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=142384&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page="
#100페이지까지 총 1000개의 리뷰
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
cat("sdf")
postdate<-postdate[c(2,4,6,8,10,12,14,16,,18,20)]
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
page<-c(1:100)
alltxt<-c()
for (i in page){
url1<-paste0(url,i)
html<-read_html(url1)
description<-html_nodes(html,css=".score_reple")%>%html_nodes("p")%>%html_text()
postdate<-html_nodes(html,css=".score_reple")%>%html_nodes("dt")%>%html_nodes("em")%>%html_text(trim=T)
postdate<-postdate[c(2,4,6,8,10,12,14,16,18,20)]
txt = cbind(postdate,description)
alltxt<-rbind(alltxt,txt)
print(i)
}
alltxt
final_dat = data.frame(alltxt, stringsAsFactors = F)
final_dat
nrow(final_dat)
length(filename)
datalist<-list()
titleset<-c()
for (i in 2:94){
a<-read_html(filename[i])
title<-html_nodes(a,"title")
title<-html_text(title)
title<-strsplit(title, split="'")[[1]][2]
b<-html_table(a)
#datalist<-append(datalist,b)
titleset<-append(titleset,title)
datalist[[title]]<-b
}
filename<-readLines("list.txt")
library(rvest)
length(filename)
datalist<-list()
titleset<-c()
for (i in 2:94){
a<-read_html(filename[i])
title<-html_nodes(a,"title")
title<-html_text(title)
title<-strsplit(title, split="'")[[1]][2]
b<-html_table(a)
#datalist<-append(datalist,b)
titleset<-append(titleset,title)
datalist[[title]]<-b
}
datalist
str(datalist)
final_dat$postdate
class(final_dat$postdate)
str(final_dat)
tb <- table(final_dat$postdate)#tb<-table(final_dat[,'postdate'])
tb
x<-strsplit(names(tb), split=" ")
x
strsplit(final_dat$postdate, split=" ")
x[[1000]]
x<-strsplit(names(tb), split=" ")
length(x)
x[[957]]
length(x[[957]][1])
class(x[[957]][1])
x[[957]][1][1]
class(x)
class(names(tb))
names(tb)
x0<-list()
for (i in 1:length(names(tb))){
x1<-x[[i]][1][1]
x<-append(x0,x1)
}
length(names(tb))
x[[1]][1][1]
x1<-x[[1]][1][1]
x0<-list()
for (i in 1:length(names(tb))){
x1<-x[[i]][1][1]
x2<-append(x0,x1)
}
x2<-append(x0,x1)
x2
x1<-x[[2]][1][1]
x[[2]][1][1]
x<-strsplit(names(tb), split=" ")
x1<-x[[2]][1][1]
x0<-list()
for (i in 1:length(names(tb))){
x1<-x[[i]][1][1]
x0<-append(x0,x1)
}
x0
x <-as.Date(x0, format = "%Y.%m.%d.")
x0<-unlist(x0)
x <-as.Date(x0, format = "%Y.%m.%d.")
x
x0
names(tb)
class(names(tb))
class(x0)
x <-as.Date(names(tb), format = "%Y.%m.%d.")
x
x0<-list()
for (i in 1:length(names(tb))){
x1<-x[[i]][1][1]
x0<-append(x0,x1)
}
x0<-unlist(x0)
library(pspline)
x <-as.Date(x0, format = "%Y.%m.%d.")
x
x0
x0
tb <- table(final_dat$postdate)#tb<-table(final_dat[,'postdate'])
x<-strsplit(names(tb), split=" ")
x0<-list()
for (i in 1:length(names(tb))){
x1<-x[[i]][1][1]
x0<-append(x0,x1)
}
x0<-unlist(x0)
x0
as.Date(x0[1], format = "%Y.%m.%d.")
as.Date(x0[1], format = "%Y%m%d")
x0[1]
class(x0[1])
as.Date(x0[1])
x <-as.Date(x0, format = "%Y.%m.%d")
x
tb
y <- as.numeric(tb)
plot(x, y, pch = 19, cex = 0.5)
fit <- sm.spline(x = as.integer(x), y = y, cv = TRUE)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
names(tb)
length(x0)
length(y)
plot(x, y, pch = 19, cex = 0.5)
lines(x=x, y=fit$ysmth, lty = 2, col = 'blue')
length(fit$ysmth)
length(as.integer(x))
fit$ysmth
head(x,1)
x[1]
as.Date(as.integer(x[1]), origin = "1970-01-01")
xx <- as.Date(as.integer(min(x)):as.integer(max(x)),origin = "1970-01-01")#0인 날까지 포함
xx
yy <- rep(0, length(xx))
xx%in%x
yy[xx%in%x] <-y
length(x)
length(y)
class(x)
class(y)
length(as.integer(x))
library(pspline)
length(fit$ysmth)
fit
head(x,1)
as.Date(as.integer(x[1]), origin = "1970-01-01")
xx <- as.Date(as.integer(min(x)):as.integer(max(x)),origin = "1970-01-01")#0인 날까지 포함
yy <- rep(0, length(xx))
yy[xx%in%x] <-y
plot(xx,yy, pch = 19, cex = 0.5)
# penalized spline function
fit<-sm.spline(xx,yy,cv = TRUE)
points(fit$x, fit$ysmth, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
length(xx)
length(yy)
xxint <- as.integer(xx)
rdata = data.frame(y = yy, x = xxint)
fit<-loess(y~x,data = rdata, span = 0.5, normalize = FALSE)
plot(fit, pch = 19, cex = 0.5)
points(fit$x,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
# K fold cross validation
k.fold = 5
idx <-sample(1:5, length(xxint), replace = TRUE)
k = 1
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr, span = 0.1, normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
mean((fit.y-rdata.va$y)^2, na.rm = T)
k.fold = 10
idx <-sample(1:k.fold, length(xxint), replace = TRUE)#replace복원 추출하겠다.
span.var <- seq(0.02, 0.5, by  = 0.01)
valid.mat <- NULL #span이 어떤 게 적당한지
for (j in 1:length(span.var))
{
valid.err <- c() #kfold for문 돌리기
for (k in 1:k.fold)
{
rdata.tr <- rdata[idx != k, ]
rdata.va <- rdata[idx == k, ]
fit<-loess(y~x,data = rdata.tr,
span = span.var[j], normalize = FALSE)
fit.y<-predict(fit, newdata = rdata.va)
valid.err[k] <- mean((fit.y-rdata.va$y)^2, na.rm = T)
}
valid.mat <- cbind(valid.mat, valid.err)
}
# check
boxplot(valid.mat)
lines(colMeans(valid.mat), col = "blue", lty = 2)
span.par<- span.var[which.min(colMeans(valid.mat))]
fit<-loess(y~x,data = rdata,
span = span.par, normalize = FALSE)
#밸리데이션 에러가 가장 작은 값을 span값으로 쓴거임.
plot(xx,yy,  pch = 19, cex = 0.5)
points(xx,fit$fitted, type = 'l', lty = 2, lwd = 1.5, col = 'blue')
final_dat[10,5]
library(KoNLP)
dim(final_dat)
extractNoun(a, autoSpacing = T)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,5]<-   gsub(pattern = "<[/|A-Za-z]*>",
replace = "", final_dat[i,5])
}
extractNoun(a, autoSpacing = T)
dat_tmp <- final_dat
for (i in 1:nrow(final_dat))
{
dat_tmp[i,2]<-   gsub(pattern = "<[/|A-Za-z]*>",
replace = "", final_dat[i,2])
}
library(tm)
install.packages("tm")
text = dat_tmp[,2]
cps = Corpus(VectorSource(text)) #coupus=말뭉치#말뭉치가 몇개인지
library(KoNLP)
cps = Corpus(VectorSource(text)) #coupus=말뭉치#말뭉치가 몇개인지
library(tm)
cps = Corpus(VectorSource(text)) #coupus=말뭉치#말뭉치가 몇개인지
dtm = tm::DocumentTermMatrix(cps,
control = list(tokenize = extractNoun,
removeNumber = T,
removePunctuation = T))
rmat <- as.matrix(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat) #각 단어에 대해 총 몇번 언급되었는지
wname <- dtm$dimnames$Terms #dim은 document와 term으로 나뉨. 고로 wname은 단어에 대한 정보
wname <- repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
sort.var <- sort(wcount,decreasing = T)[100]
sort.var
wcount
dtm$dimnames
rmat
dtm
dtm$dimnames
wname <- repair_encoding(dtm$dimnames$Terms)
colnames(rmat)<- wname
colnames(rmat)
wname <- dtm$dimnames$Terms #dim은 document와 term으로 나뉨. 고로 wname은 단어에 대한 정보
dtm$dimnames
Encoding(wname)='UTF-8'
wname
guess_encoding(dtm)
guess_encoding(dtm$dimnames$Terms)
dtm = tm::DocumentTermMatrix(cps,
control = list(tokenize = extractNoun,
removeNumber = T,
wordLengths=c(1,10),
removePunctuation = T))
rmat <- as.matrix(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat) #각 단어에 대해 총 몇번 언급되었는지
wname <- dtm$dimnames$Terms #dim은 document와 term으로 나뉨. 고로 wname은 단어에 대한 정보
wname <- repair_encoding(dtm$dimnames$Terms)#Encoding(wname)='UTF-8'
wname
Encoding(wname)='UTF-8'
wname
cps = Corpus(DataframeSource(data.frame(text)))
data.frame(text)
text<-iconv(text,"UTF-8","latin1")
cps = Corpus(VectorSource(text)) #coupus=말뭉치#말뭉치가 몇개인지
dtm = tm::DocumentTermMatrix(cps,
control = list(tokenize = extractNoun,
removeNumber = T,
wordLengths=c(1,10),
removePunctuation = T))
dtm$dimnames
dtm$dimnames$Terms
text<-iconv(text,"UTF-8")
text = dat_tmp[,2]
text<-iconv(text,"UTF-8")
cps = Corpus(VectorSource(text)) #coupus=말뭉치#말뭉치가 몇개인지
dtm = tm::DocumentTermMatrix(cps,
control = list(tokenize = extractNoun,
removeNumber = T,
wordLengths=c(1,10),
removePunctuation = T))
dtm$dimnames$Terms
rmat <- as.matrix(dtm)
library(Matrix)
rmat <-spMatrix(dtm$nrow,dtm$ncol, i=dtm$i, j=dtm$j, x=dtm$v)
wcount<-colSums(rmat) #각 단어에 대해 총 몇번 언급되었는지
wname <- dtm$dimnames$Terms #dim은 document와 term으로 나뉨. 고로 wname은 단어에 대한 정보
colnames(rmat)<- wname
wname
sort.var <- sort(wcount,decreasing = T)[100]
sort.var
wcount
View(wname)
idx <- !( grepl(query.n, wname)| (wcount<=sort.var)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname))
idx <- !( grepl(query, wname)| (wcount<=sort.var)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname))
query='공조'
idx <- !( grepl(query, wname)| (wcount<=sort.var)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx] #아임리얼 제외, 언급된 횟수 24보다 작은 것 제외한 단어에 대한 언급수
library(wordcloud)
wordcloud(wname.rel,freq = wcount.rel)
idx <- !( grepl(query, wname)| (wcount<=sort.var)|grepl('정말',wname)|grepl('진짜',wname)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx] #아임리얼 제외, 언급된 횟수 24보다 작은 것 제외한 단어에 대한 언급수
library(wordcloud)
wordcloud(wname.rel,freq = wcount.rel)
idx <- !( grepl(query, wname)| (wcount<=sort.var)|grepl('너무',wname)|grepl('정말',wname)|grepl('진짜',wname)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname)grepl('ㅎㅎ',wname)|)
wname.rel <- wname[idx]
wcount.rel <- wcount[idx] #아임리얼 제외, 언급된 횟수 24보다 작은 것 제외한 단어에 대한 언급수
wordcloud(wname.rel,freq = wcount.rel)
idx <- !( grepl(query, wname)|(wcount<=sort.var)|grepl('영화',wname)|grepl('너무',wname)|grepl('정말',wname)|grepl('진짜',wname)|grepl('ㅋㅋㅋ',wname)|grepl('ㅋㅋㅋㅋ',wname)|grepl('ㅋㅋ',wname)|grepl('ㅋ',wname)|grepl('ㅎㅎ',wname))
wname.rel <- wname[idx]
wcount.rel <- wcount[idx] #아임리얼 제외, 언급된 횟수 24보다 작은 것 제외한 단어에 대한 언급수
wordcloud(wname.rel,freq = wcount.rel)
pal <- brewer.pal(9, "Set1") #컬러 설정
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
dtm = as.matrix(dtm)
wordcloud(wname.rel,freq = wcount.rel, colors = pal)
word.c<-order(wcount.rel,decreasing=TRUE)
freq.words <- rmat[word.c, ]
co.matrix<-freq.words%*%t(freq.words)
rowSums(dtm)
word.order<-order(wcount.rel,decreasing=TRUE)
head(rmat)
freq.words <- rmat[word.order[1:30], ]
co.matrix<-freq.words%*%t(freq.words)
co.matrix
freq.words <- rmat[word.order[1:50], ]
co.matrix<-freq.words%*%t(freq.words)
freq.words
library(qgraph)
par(family="Apple SD Gothic Neo")
qgraph(co.matrix, labels=rownames(co.matrix),
diag=FALSE, layout='spring', threshold=3,
vsize=log(diag(co.matrix)) * 2)
install.packages("qgraph")
library(qgraph)
par(family="Apple SD Gothic Neo")
qgraph(co.matrix, labels=rownames(co.matrix),
diag=FALSE, layout='spring', threshold=3,
vsize=log(diag(co.matrix)) * 2)
library("rjson")
install.packages("rjson")
filename
titleset
head(final_dat)
head(datalist)
titleset
View(titleset)
names(datalist)
